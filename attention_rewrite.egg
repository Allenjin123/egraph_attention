; ============================================================
; Attention Algorithm Rewrite Rules
; ============================================================
; Transform 3-pass attention into 2-pass tiled attention
; using compositional rewrite rules enabled by split M/R design
; ============================================================

(datatype Tensor
  (CreateTensor i64 i64 String :cost 0)

  ; Tiling operations - expensive (favor non-tiled 3-pass)
  (T_split_m_m1m0 Tensor i64 :cost 100)    ; Split m into tiles = expensive
  (T_unsplit_m1m0_m Tensor :cost 100)      ; Combine tiles back = expensive

  ; Map operations (element-wise, no reduction)
  (M_mul_emp Tensor Tensor :cost 1)        ; Non-tiled = cheap (3-pass)
  (M_mul_em1m0p Tensor Tensor :cost 10)    ; Tiled = expensive
  (M_mul_fmp Tensor Tensor :cost 1)        ; Multiply over {f,m,p}
  (M_sub_mp Tensor Tensor :cost 1)         ; Non-tiled = cheap (3-pass)
  (M_sub_m1m0p Tensor Tensor :cost 10)     ; Tiled = expensive
  (M_sub_m1p Tensor Tensor :cost 10)       ; Correction factor = expensive
  (M_exp_mp Tensor :cost 1)                ; Non-tiled = cheap (3-pass)
  (M_exp_m1m0p Tensor :cost 10)            ; Tiled = expensive
  (M_exp_m1p Tensor :cost 10)              ; Correction factor = expensive
  (M_div_mp Tensor Tensor :cost 1)         ; In-loop division = cheap (3-pass)
  (M_div_m1m0p Tensor Tensor :cost 50)     ; Tiled in-loop division = expensive
  (M_mul_m1m0p Tensor Tensor :cost 10)     ; Correction multiply = expensive
  (M_mul_m1p Tensor Tensor :cost 10)       ; Correction multiply = expensive
  (M_div_fp Tensor Tensor :cost 100)       ; Post-loop division = expensive (2-pass)

  ; Reduce operations (collapse dimension)
  (R_add_e Tensor :cost 1)                 ; Sum over e dimension
  (R_add_m Tensor :cost 1)                 ; Global reduction = cheap (3-pass)
  (R_add_m0 Tensor :cost 10)               ; Local reduction = expensive
  (R_add_m1 Tensor :cost 100)              ; Cross-tile reduction = very expensive
  (R_max_m Tensor :cost 1)                 ; Global reduction = cheap (3-pass)
  (R_max_m0 Tensor :cost 10)               ; Local reduction = expensive
  (R_max_m1 Tensor :cost 100)              ; Cross-tile reduction = very expensive
)

; ============================================================
; CORE REWRITE RULES
; ============================================================
; These rules transform 3-pass attention to 2-pass by:
; 1. Inserting tiling of K
; 2. Converting global reductions to local-then-global
; 3. Computing correction factors for numerical stability
; 4. Using post-loop normalization
;
; Strategy: First add tiled versions WITHOUT subsume, then use
; rules to construct the 2-pass pattern, then extract optimal.
; ============================================================

; Rule 1: Insert tiling of K in matrix multiplication
; Creates BK (tiled K) and BQK_temp (tiled Q*K)
(rewrite
  (M_mul_emp q k)
  (M_mul_em1m0p q (T_split_m_m1m0 k 4)))

; Give names to tiled tensors
(rule
  ((= bk (T_split_m_m1m0 k 4)))
  ((let BK bk))
)

(rule
  ((= bqk_temp (M_mul_em1m0p q bk)))
  ((let BQK_temp bqk_temp))
)

; Rule 2: Transform global max to local-then-global pattern
(rewrite
  (R_max_m qk)
  (R_max_m1 (R_max_m0 qk)))

; Rule 3: Transform global sum to local-then-global pattern
(rewrite
  (R_add_m exp_vals)
  (R_add_m1 (R_add_m0 exp_vals)))

; Rule 4: Adapt exp for tiled dimensions
(rewrite
  (M_exp_mp shifted)
  (M_exp_m1m0p shifted))

; Rule 5: Subtraction uses LOCAL max for numerical stability
; This enables the 2-pass structure where correction is applied later
(rewrite
  (M_sub_mp qk gm)
  (M_sub_m1m0p qk (R_max_m0 qk)))

; Give names to 2-pass intermediate tensors
(rule
  ((= lm (R_max_m0 bqk)))
  ((let LM lm))
)

(rule
  ((= bqk_shifted (M_sub_m1m0p bqk lm)))
  ((let BQK_shifted bqk_shifted))
)

(rule
  ((= sln (M_exp_m1m0p shifted)))
  ((let SLN sln))
)

; Rule 6: Transform division to tiled version
(rewrite
  (M_div_mp sn sd)
  (M_div_m1m0p sn sd))

; Run basic rewrites first
(run 10)

; Simpler approach: use a rule to first create correction structures
; Then use another rewrite to transform the output

; Step 1: Whenever we have local softmax, create correction factor
; Uses proper names for 2-pass intermediate tensors
(rule
  ((= sln (M_exp_m1m0p (M_sub_m1m0p bqk lm)))
   (= lm (R_max_m0 bqk)))
  ((let GM (R_max_m1 lm))
   (let LM_GM_diff (M_sub_m1p lm GM))
   (let PRM (M_exp_m1p LM_GM_diff))
   (let CN (M_mul_m1m0p sln PRM))
   (let SLD (R_add_m0 sln))
   (let CD (M_mul_m1p SLD PRM))
   (let GD (R_add_m1 CD)))
)

(run 10)

; Step 2: Transform output when correction exists
; Match: any expression using M_div_m1m0p and M_mul_fmp with V
; Uses proper names for 2-pass output tensors
(rule
  ((= output (R_add_m1 (R_add_m0 (M_mul_fmp div_result v))))
   (= div_result (M_div_m1m0p sln sum))
   (= sln (M_exp_m1m0p (M_sub_m1m0p bqk lm)))
   (= lm (R_max_m0 bqk))
   (= cn (M_mul_m1m0p sln prm))
   (= gd (R_add_m1 cd)))
  ((let A_unnorm (T_unsplit_m1m0_m cn))
   (let AV_temp (M_mul_fmp A_unnorm v))
   (let AV_acc (R_add_m AV_temp))
   (let AV (M_div_fp AV_acc gd))
   (union output AV))
)

; ============================================================
; EXAMPLE: 3-Pass Attention
; ============================================================
; Define the standard 3-pass attention algorithm
; The rewrite rules will automatically transform it to 2-pass
; ============================================================

; Create input tensors
(let Q (CreateTensor 64 1024 "Q"))   ; Query: {e,p}
(let K (CreateTensor 64 1024 "K"))   ; Key: {e,m}
(let V (CreateTensor 64 1024 "V"))   ; Value: {f,m}

; Step 1: Compute QK = Q @ K^T
(let QK_temp (M_mul_emp Q K))        ; Map: element-wise multiply
(let QK (R_add_e QK_temp))           ; Reduce: sum over e dimension

; Step 2: Compute global max for numerical stability
(let GM (R_max_m QK))                ; Max over m dimension

; Step 3: Compute softmax numerator
(let QK_shifted (M_sub_mp QK GM))    ; Subtract max for stability
(let SN (M_exp_mp QK_shifted))       ; Exponential

; Step 4: Compute softmax denominator
(let SD (R_add_m SN))                ; Sum over m dimension

; Step 5: Normalize to get attention weights
(let A (M_div_mp SN SD))             ; Element-wise division

; Step 6: Compute final output
(let AV_temp (M_mul_fmp A V))        ; Map: multiply A and V
(let AV (R_add_m AV_temp))           ; Reduce: sum over m dimension

; ============================================================
; RUN FINAL TRANSFORMATION
; ============================================================

; Apply final rules
(run 20)

; Extract final result - should show 2-pass version with M_div_fp at end
(extract AV)

; ============================================================
; WHAT THE REWRITE RULES DO:
; ============================================================
;
; The rewrite rules automatically transform the 3-pass algorithm to:
;
; 1. Tile K: BK = T_split_m_m1m0(K, 4)
;
; 2. Compute tiled QK:
;    BQK_temp = M_mul_em1m0p(Q, BK)
;    BQK = R_add_e(BQK_temp)
;
; 3. Local max per tile:
;    LM = R_max_m0(BQK)
;
; 4. Global max (synchronization barrier):
;    GM = R_max_m1(LM)
;
; 5. Local softmax with local max:
;    SLN = M_exp_m1m0p(M_sub_m1m0p(BQK, LM))
;    SLD = R_add_m0(SLN)
;
; 6. Correction factor:
;    PRM = M_exp_m1p(M_sub_m1p(LM, GM))
;
; 7. Corrected statistics:
;    CN = M_mul_m1m0p(SLN, PRM)
;    CD = M_mul_m1p(SLD, PRM)
;
; 8. Global denominator:
;    GD = R_add_m1(CD)
;
; 9. Normalize with global denominator:
;    A_tiled = M_div_m1m0p(CN, GD)
;
; 10. Untile and compute output:
;     A = T_unsplit_m1m0_m(A_tiled)
;     AV = A @ V
;
; This achieves the same result as 3-pass but with better memory efficiency!
; ============================================================
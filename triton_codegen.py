"""
Triton Code Generator for Egglog Attention Algorithms

Generates executable Triton/PyTorch code from the parsed computation graph.
Two strategies:
1. Composable: Separate kernels for each operation (easier to debug, more flexible)
2. Fused: Single kernel for the full computation (better performance)
"""

import torch
import triton
import triton.language as tl
from typing import Dict, List, Optional
from dataclasses import dataclass

from egg_parser import ComputationGraph, IRNode
from egg_ops import OP_REGISTRY, DimSpec, get_op_spec


# ============================================================================
# Code Generation Templates
# ============================================================================

IMPORTS_TEMPLATE = '''"""
Auto-generated attention implementation from egglog.
Generated by: egg_to_triton.py
"""

import torch
import triton
import triton.language as tl
import math
from typing import Optional

'''

# ============================================================================
# PyTorch Reference Implementation (for verification)
# ============================================================================

class PyTorchCodeGen:
    """Generate PyTorch reference implementation for correctness verification."""

    def __init__(self, graph: ComputationGraph):
        self.graph = graph

    def generate(self) -> str:
        """Generate PyTorch implementation."""
        code = [IMPORTS_TEMPLATE]
        code.append(self._generate_attention_function())
        return '\n'.join(code)

    def _generate_attention_function(self) -> str:
        """Generate the main attention function."""
        lines = []
        lines.append("def egg_attention_pytorch(Q, K, V, scale=None):")
        lines.append('    """')
        lines.append('    Attention implementation generated from egglog.')
        lines.append('    Uses PyTorch operations for correctness verification.')
        lines.append('    ')
        lines.append('    Args:')
        lines.append('        Q: Query tensor [head_dim, seq_len] or [batch, heads, seq, dim]')
        lines.append('        K: Key tensor [head_dim, seq_len]')
        lines.append('        V: Value tensor [head_dim, seq_len]')
        lines.append('        scale: Scaling factor (default: 1/sqrt(head_dim))')
        lines.append('    """')
        lines.append('    # Handle different input formats')
        lines.append('    if Q.dim() == 4:')
        lines.append('        # [batch, heads, seq, dim] -> process each head')
        lines.append('        batch, heads, seq_len, head_dim = Q.shape')
        lines.append('        Q = Q.permute(0, 1, 3, 2)  # [batch, heads, dim, seq]')
        lines.append('        K = K.permute(0, 1, 3, 2)')
        lines.append('        V = V.permute(0, 1, 3, 2)')
        lines.append('        # Reshape for processing')
        lines.append('        Q = Q.reshape(-1, head_dim, seq_len)  # [batch*heads, dim, seq]')
        lines.append('        K = K.reshape(-1, head_dim, seq_len)')
        lines.append('        V = V.reshape(-1, head_dim, seq_len)')
        lines.append('        multi_head = True')
        lines.append('    else:')
        lines.append('        # Assume [dim, seq] format')
        lines.append('        head_dim, seq_len = Q.shape')
        lines.append('        batch, heads = 1, 1')
        lines.append('        Q = Q.unsqueeze(0)')
        lines.append('        K = K.unsqueeze(0)')
        lines.append('        V = V.unsqueeze(0)')
        lines.append('        multi_head = False')
        lines.append('')
        lines.append('    if scale is None:')
        lines.append('        scale = 1.0 / math.sqrt(head_dim)')
        lines.append('')

        # Generate operations in execution order
        lines.append('    # ===== Computation Graph =====')

        for node_id in self.graph.execution_order:
            node = self.graph.nodes[node_id]
            if node.is_primitive() or node.op == 'CreateTensor':
                continue

            op_code = self._generate_op(node)
            if op_code:
                lines.append(f'    {op_code}')

        # Return the output
        if self.graph.outputs:
            out_var = self.graph.outputs[0].var_name
            lines.append('')
            lines.append('    # ===== Output =====')
            lines.append('    if multi_head:')
            lines.append(f'        # Reshape back to [batch, heads, seq, dim]')
            lines.append(f'        out = {out_var}.reshape(batch, heads, head_dim, seq_len)')
            lines.append(f'        out = out.permute(0, 1, 3, 2)')
            lines.append(f'        return out')
            lines.append('    else:')
            lines.append(f'        return {out_var}.squeeze(0)')

        return '\n'.join(lines)

    def _generate_op(self, node: IRNode) -> str:
        """Generate code for a single operation."""
        op = node.op
        var = node.var_name
        children = [self.graph.nodes[c].var_name if c in self.graph.nodes else c
                    for c in node.children]

        if op == 'M_mul_emp':
            # Q[e,p] * K[e,m] -> temp[e,m,p]
            # In our layout: Q[batch, e, p], K[batch, e, m]
            return f'{var} = {children[0]}.unsqueeze(2) * {children[1]}.unsqueeze(3)  # [batch, e, m, p]'

        elif op == 'R_add_e':
            # Sum over e dimension -> [batch, m, p]
            return f'{var} = {children[0]}.sum(dim=1)  # reduce e -> [batch, m, p]'

        elif op == 'R_max_m':
            # Max over m dimension -> [batch, p]
            return f'{var} = {children[0]}.max(dim=1)[0]  # reduce m -> [batch, p]'

        elif op == 'M_sub_mp':
            # A[m,p] - B[p] -> [m,p]
            return f'{var} = {children[0]} - {children[1]}.unsqueeze(1)  # broadcast subtract'

        elif op == 'M_exp_mp':
            # exp(A[m,p])
            return f'{var} = torch.exp({children[0]})'

        elif op == 'R_add_m':
            # Sum over m dimension -> [batch, p] or [batch, f, p]
            return f'{var} = {children[0]}.sum(dim=-2)  # reduce m'

        elif op == 'M_div_mp':
            # A[m,p] / B[p]
            return f'{var} = {children[0]} / {children[1]}.unsqueeze(-2)  # broadcast divide'

        elif op == 'M_mul_fmp':
            # A[m,p] * V[f,m] -> [f,m,p]
            # A is [batch, m, p], V is [batch, f, m]
            return f'{var} = {children[0]}.unsqueeze(1) * {children[1]}.unsqueeze(3)  # [batch, f, m, p]'

        # Tiled operations (for 2-pass attention)
        elif op == 'T_split_m_m1m0':
            # Split m into m1 x m0 tiles
            return f'# {var} = split_tiles({children[0]})  # TODO: implement tiling'

        elif op == 'T_unsplit_m1m0_m':
            # Merge tiles
            return f'# {var} = merge_tiles({children[0]})  # TODO: implement untiling'

        else:
            # Unknown op - generate placeholder
            args = ', '.join(children)
            return f'# {var} = {op}({args})  # TODO: implement'

        return None


# ============================================================================
# Triton Kernel Templates
# ============================================================================

TRITON_KERNEL_MATMUL_REDUCE = '''
@triton.jit
def _matmul_reduce_kernel(
    Q_ptr, K_ptr, Out_ptr,
    E: tl.constexpr, M: tl.constexpr, P: tl.constexpr,
    stride_qe, stride_qp,
    stride_ke, stride_km,
    stride_om, stride_op,
    BLOCK_M: tl.constexpr, BLOCK_P: tl.constexpr, BLOCK_E: tl.constexpr,
):
    """Compute QK = sum_e(Q[e,p] * K[e,m]) -> [m,p]"""
    pid_m = tl.program_id(0)
    pid_p = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_e = tl.arange(0, BLOCK_E)

    # Initialize accumulator
    acc = tl.zeros([BLOCK_M, BLOCK_P], dtype=tl.float32)

    # Loop over E dimension
    for e_start in range(0, E, BLOCK_E):
        e_offs = e_start + offs_e

        # Load Q[e, p] and K[e, m]
        q_ptrs = Q_ptr + e_offs[:, None] * stride_qe + offs_p[None, :] * stride_qp
        k_ptrs = K_ptr + e_offs[:, None] * stride_ke + offs_m[None, :] * stride_km

        q = tl.load(q_ptrs, mask=(e_offs[:, None] < E) & (offs_p[None, :] < P), other=0.0)
        k = tl.load(k_ptrs, mask=(e_offs[:, None] < E) & (offs_m[None, :] < M), other=0.0)

        # Accumulate: sum_e(Q[e,p] * K[e,m]) -> [m,p]
        # q: [BLOCK_E, BLOCK_P], k: [BLOCK_E, BLOCK_M]
        # We want: for each (m,p), sum over e of Q[e,p] * K[e,m]
        acc += tl.dot(tl.trans(k), q)  # [BLOCK_M, BLOCK_P]

    # Store output
    out_ptrs = Out_ptr + offs_m[:, None] * stride_om + offs_p[None, :] * stride_op
    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_p[None, :] < P))
'''


# ============================================================================
# Composable Code Generator
# ============================================================================

class ComposableCodeGen:
    """Generate composable Triton/PyTorch code."""

    def __init__(self, graph: ComputationGraph, use_triton: bool = False):
        self.graph = graph
        self.use_triton = use_triton

    def generate(self) -> str:
        """Generate composable implementation."""
        code = [IMPORTS_TEMPLATE]

        if self.use_triton:
            code.append("# ===== Triton Kernels =====")
            code.append(TRITON_KERNEL_MATMUL_REDUCE)
            code.append("")

        # Generate the main function (PyTorch-based for now)
        pytorch_gen = PyTorchCodeGen(self.graph)
        code.append(pytorch_gen._generate_attention_function())

        # Generate a simple test
        code.append(self._generate_test())

        return '\n'.join(code)

    def _generate_test(self) -> str:
        """Generate test code."""
        return '''

# ===== Test =====
if __name__ == "__main__":
    import torch

    # Test with simple inputs
    torch.manual_seed(42)
    head_dim = 64
    seq_len = 128

    Q = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    K = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    V = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)

    # Run generated attention
    out = egg_attention_pytorch(Q, K, V)
    print(f"Output shape: {out.shape}")

    # Compare with reference
    scale = 1.0 / (head_dim ** 0.5)
    scores = torch.matmul(Q.T, K) * scale  # [seq, seq]
    attn = torch.softmax(scores, dim=-1)
    ref = torch.matmul(attn, V.T).T  # [dim, seq]

    diff = (out - ref).abs().max().item()
    print(f"Max diff vs reference: {diff:.6f}")
    if diff < 0.01:
        print("PASSED!")
    else:
        print("FAILED - outputs differ")
'''


# ============================================================================
# Full Attention Triton Kernel (Fused)
# ============================================================================

class FusedCodeGen:
    """Generate a single fused Triton kernel for the full attention."""

    def __init__(self, graph: ComputationGraph):
        self.graph = graph

    def generate(self) -> str:
        """Generate fused Triton implementation."""
        code = [IMPORTS_TEMPLATE]

        # For now, generate a template that matches the 3-pass structure
        code.append(self._generate_fused_kernel())
        code.append(self._generate_wrapper())
        code.append(self._generate_test())

        return '\n'.join(code)

    def _generate_fused_kernel(self) -> str:
        """Generate the fused Triton kernel."""
        return '''
# ===== Fused 3-Pass Attention Kernel =====
# Generated from egglog computation graph

@triton.jit
def _egg_attention_kernel(
    Q_ptr, K_ptr, V_ptr, Out_ptr,
    stride_qb, stride_qh, stride_qm, stride_qk,
    stride_kb, stride_kh, stride_kn, stride_kk,
    stride_vb, stride_vh, stride_vn, stride_vk,
    stride_ob, stride_oh, stride_om, stride_ok,
    batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
    scale,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Fused attention kernel generated from egglog 3-pass representation.

    Implements:
    1. QK = sum_e(Q[e,p] * K[e,m]) * scale  -> [m,p]
    2. GM = max_m(QK)                        -> [p]
    3. SN = exp(QK - GM)                     -> [m,p]
    4. SD = sum_m(SN)                        -> [p]
    5. A = SN / SD                           -> [m,p]
    6. Out = sum_m(A * V)                    -> [f,p]
    """
    pid_batch = tl.program_id(0)
    pid_head = tl.program_id(1)
    pid_m = tl.program_id(2)

    # Query block offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_k = tl.arange(0, BLOCK_K)

    # Load Q block
    q_ptrs = Q_ptr + (pid_batch * stride_qb + pid_head * stride_qh +
                      offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)
    q_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    q = tl.load(q_ptrs, mask=q_mask, other=0.0)

    # Initialize for 3-pass
    row_max = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)

    # ===== PASS 1: Compute QK and find row-wise max =====
    num_blocks_n = tl.cdiv(seq_len_k, BLOCK_N)
    for block_n in range(num_blocks_n):
        start_n = block_n * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K block
        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # QK = Q @ K^T * scale
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale

        # Causal mask (optional)
        # qk = tl.where(offs_m[:, None] >= offs_n[None, :], qk, float('-inf'))
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Update row max
        block_max = tl.max(qk, axis=1)
        row_max = tl.maximum(row_max, block_max)

    # ===== PASS 2: Compute exp sum =====
    row_sum = tl.zeros([BLOCK_M], dtype=tl.float32)
    for block_n in range(num_blocks_n):
        start_n = block_n * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        p = tl.exp(qk - row_max[:, None])
        row_sum += tl.sum(p, axis=1)

    # ===== PASS 3: Compute output =====
    acc = tl.zeros([BLOCK_M, BLOCK_K], dtype=tl.float32)
    for block_n in range(num_blocks_n):
        start_n = block_n * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Normalized attention weights
        p = tl.exp(qk - row_max[:, None]) / row_sum[:, None]

        # Load V and accumulate
        v_ptrs = V_ptr + (pid_batch * stride_vb + pid_head * stride_vh +
                          offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)
        v_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        v = tl.load(v_ptrs, mask=v_mask, other=0.0)

        acc += tl.dot(p.to(v.dtype), v)

    # Store output
    out_ptrs = Out_ptr + (pid_batch * stride_ob + pid_head * stride_oh +
                          offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)
    out_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    tl.store(out_ptrs, acc.to(Out_ptr.dtype.element_ty), mask=out_mask)
'''

    def _generate_wrapper(self) -> str:
        """Generate Python wrapper function."""
        return '''

def egg_attention_triton(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    scale: float = None,
) -> torch.Tensor:
    """
    Attention implementation generated from egglog (Triton kernel).

    Args:
        q: Query tensor [batch, heads, seq_q, dim]
        k: Key tensor [batch, heads, seq_k, dim]
        v: Value tensor [batch, heads, seq_k, dim]
        scale: Scaling factor (default: 1/sqrt(dim))

    Returns:
        Output tensor [batch, heads, seq_q, dim]
    """
    batch_size, num_heads, seq_len_q, head_dim = q.shape
    _, _, seq_len_k, _ = k.shape

    if scale is None:
        scale = 1.0 / math.sqrt(head_dim)

    out = torch.empty_like(q)

    # Block sizes
    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = triton.next_power_of_2(head_dim)

    # Grid
    grid = (batch_size, num_heads, triton.cdiv(seq_len_q, BLOCK_M))

    _egg_attention_kernel[grid](
        q, k, v, out,
        q.stride(0), q.stride(1), q.stride(2), q.stride(3),
        k.stride(0), k.stride(1), k.stride(2), k.stride(3),
        v.stride(0), v.stride(1), v.stride(2), v.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
        scale,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return out
'''

    def _generate_test(self) -> str:
        """Generate test code."""
        return '''

# ===== Test =====
if __name__ == "__main__":
    import torch

    torch.manual_seed(42)
    batch_size = 1
    num_heads = 4
    seq_len = 256
    head_dim = 64

    Q = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float16)
    K = torch.randn_like(Q)
    V = torch.randn_like(Q)

    # Run generated attention
    out = egg_attention_triton(Q, K, V)
    print(f"Output shape: {out.shape}")

    # Compare with PyTorch reference
    scale = 1.0 / math.sqrt(head_dim)
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1, dtype=torch.float32).to(Q.dtype)
    ref = torch.matmul(attn, V)

    diff = (out - ref).abs().max().item()
    print(f"Max diff vs reference: {diff:.6f}")
    if diff < 0.01:
        print("PASSED!")
    else:
        print("FAILED - outputs differ")
'''


# ============================================================================
# 2-Pass Fused Kernel (Tiled Attention)
# ============================================================================

class TwoPassCodeGen:
    """Generate 2-pass tiled Triton kernel for attention."""

    def __init__(self, graph: ComputationGraph):
        self.graph = graph

    def generate(self) -> str:
        """Generate 2-pass Triton implementation."""
        code = [IMPORTS_TEMPLATE]
        code.append(self._generate_2pass_kernel())
        code.append(self._generate_wrapper())
        code.append(self._generate_test())
        return '\n'.join(code)

    def _generate_2pass_kernel(self) -> str:
        """Generate the 2-pass fused Triton kernel."""
        return '''
# ===== 2-Pass Tiled Attention Kernel =====
# Generated from egglog computation graph (FuseMax style)
#
# Pass 1: Process tiles, compute local max/sum
# Barrier: Compute global max from local maxes
# Pass 2: Apply correction factor, compute output

@triton.jit
def _egg_attention_2pass_kernel(
    Q_ptr, K_ptr, V_ptr, Out_ptr,
    stride_qb, stride_qh, stride_qm, stride_qk,
    stride_kb, stride_kh, stride_kn, stride_kk,
    stride_vb, stride_vh, stride_vn, stride_vk,
    stride_ob, stride_oh, stride_om, stride_ok,
    batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
    scale,
    NUM_TILES: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,  # Tile size (m0)
    BLOCK_K: tl.constexpr,
):
    """
    2-Pass attention kernel with tiling (FuseMax algorithm).

    Pass 1 (per tile):
    - BQK = Q @ K_tile^T                    -> [BLOCK_M, BLOCK_N]
    - LM = max(BQK, axis=1)                 -> [BLOCK_M] (local max)
    - SLN = exp(BQK - LM)                   -> [BLOCK_M, BLOCK_N]
    - SLD = sum(SLN, axis=1)                -> [BLOCK_M] (local sum)

    Barrier:
    - GM = max(LM across all tiles)         -> [BLOCK_M] (global max)

    Pass 2 (correction):
    - PRM = exp(LM - GM)                    -> correction factor per tile
    - CN = SLN * PRM                        -> corrected numerator
    - CD = SLD * PRM                        -> corrected denominator
    - GD = sum(CD across tiles)             -> [BLOCK_M] (global denominator)
    - A = CN / GD                           -> normalized attention
    - Out += A @ V_tile                     -> accumulate output
    """
    pid_batch = tl.program_id(0)
    pid_head = tl.program_id(1)
    pid_m = tl.program_id(2)

    # Query block offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_k = tl.arange(0, BLOCK_K)

    # Load Q block [BLOCK_M, BLOCK_K]
    q_ptrs = Q_ptr + (pid_batch * stride_qb + pid_head * stride_qh +
                      offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)
    q_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    q = tl.load(q_ptrs, mask=q_mask, other=0.0)

    # Storage for per-tile local statistics
    # We need to store local_max and local_sum for each tile
    # Since we can't have dynamic arrays in Triton, we compute in two passes

    # ===== PASS 1: Compute local statistics for each tile =====
    # Also compute global max across tiles

    global_max = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)

    # First, find global max by iterating through all tiles
    for tile_idx in range(NUM_TILES):
        start_n = tile_idx * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K tile
        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # Compute QK for this tile
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Local max for this tile
        local_max = tl.max(qk, axis=1)  # [BLOCK_M]

        # Update global max
        global_max = tl.maximum(global_max, local_max)

    # ===== PASS 2: Compute output using global max for correction =====
    global_sum = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_K], dtype=tl.float32)

    for tile_idx in range(NUM_TILES):
        start_n = tile_idx * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K tile (again - this is the "2-pass" part)
        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # Recompute QK
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Compute local max for this tile
        local_max = tl.max(qk, axis=1)  # [BLOCK_M]

        # Local softmax with local max (numerically stable within tile)
        local_exp = tl.exp(qk - local_max[:, None])  # [BLOCK_M, BLOCK_N]
        local_sum = tl.sum(local_exp, axis=1)       # [BLOCK_M]

        # Correction factor: exp(local_max - global_max)
        correction = tl.exp(local_max - global_max)  # [BLOCK_M]

        # Corrected sum (for global denominator)
        corrected_sum = local_sum * correction
        global_sum += corrected_sum

        # Load V tile
        v_ptrs = V_ptr + (pid_batch * stride_vb + pid_head * stride_vh +
                          offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)
        v_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        v = tl.load(v_ptrs, mask=v_mask, other=0.0)

        # Corrected attention weights (not normalized yet)
        # We'll normalize at the end by dividing by global_sum
        corrected_exp = local_exp * correction[:, None]  # [BLOCK_M, BLOCK_N]

        # Accumulate: corrected_exp @ V
        acc += tl.dot(corrected_exp.to(v.dtype), v)

    # Final normalization by global sum
    acc = acc / global_sum[:, None]

    # Store output
    out_ptrs = Out_ptr + (pid_batch * stride_ob + pid_head * stride_oh +
                          offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)
    out_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    tl.store(out_ptrs, acc.to(Out_ptr.dtype.element_ty), mask=out_mask)
'''

    def _generate_wrapper(self) -> str:
        """Generate Python wrapper function."""
        return '''

def egg_attention_2pass_triton(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    scale: float = None,
) -> torch.Tensor:
    """
    2-Pass attention implementation generated from egglog (Triton kernel).

    This uses the FuseMax algorithm:
    - Pass 1: Compute local max across all tiles to get global max
    - Pass 2: Use global max for numerically stable softmax, accumulate output

    Args:
        q: Query tensor [batch, heads, seq_q, dim]
        k: Key tensor [batch, heads, seq_k, dim]
        v: Value tensor [batch, heads, seq_k, dim]
        scale: Scaling factor (default: 1/sqrt(dim))

    Returns:
        Output tensor [batch, heads, seq_q, dim]
    """
    batch_size, num_heads, seq_len_q, head_dim = q.shape
    _, _, seq_len_k, _ = k.shape

    if scale is None:
        scale = 1.0 / math.sqrt(head_dim)

    out = torch.empty_like(q)

    # Block sizes - use fixed sizes for consistent shared memory usage
    BLOCK_M = 64
    BLOCK_N = 64  # Fixed tile size to avoid shared memory issues
    BLOCK_K = triton.next_power_of_2(head_dim)

    # Calculate number of tiles based on fixed BLOCK_N
    num_tiles = triton.cdiv(seq_len_k, BLOCK_N)

    # Grid
    grid = (batch_size, num_heads, triton.cdiv(seq_len_q, BLOCK_M))

    _egg_attention_2pass_kernel[grid](
        q, k, v, out,
        q.stride(0), q.stride(1), q.stride(2), q.stride(3),
        k.stride(0), k.stride(1), k.stride(2), k.stride(3),
        v.stride(0), v.stride(1), v.stride(2), v.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
        scale,
        NUM_TILES=num_tiles,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return out
'''

    def _generate_test(self) -> str:
        """Generate test code."""
        return '''

# ===== Test =====
if __name__ == "__main__":
    import torch

    torch.manual_seed(42)
    batch_size = 1
    num_heads = 4
    seq_len = 256
    head_dim = 64

    Q = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float16)
    K = torch.randn_like(Q)
    V = torch.randn_like(Q)

    # Run generated 2-pass attention
    out = egg_attention_2pass_triton(Q, K, V)
    print(f"Output shape: {out.shape}")

    # Compare with PyTorch reference
    scale = 1.0 / math.sqrt(head_dim)
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1, dtype=torch.float32).to(Q.dtype)
    ref = torch.matmul(attn, V)

    diff = (out - ref).abs().max().item()
    print(f"Max diff vs reference: {diff:.6f}")
    if diff < 0.01:
        print("PASSED!")
    else:
        print("FAILED - outputs differ")
'''


# ============================================================================
# Main Entry Point
# ============================================================================

def detect_algorithm(graph: ComputationGraph) -> str:
    """Detect whether the graph represents 3-pass or 2-pass attention."""
    ops = graph.unique_ops

    # 2-pass indicators: tiled operations
    tiled_ops = {'T_split_m_m1m0', 'T_unsplit_m1m0_m', 'R_max_m0', 'R_max_m1',
                 'R_add_m0', 'R_add_m1', 'M_sub_m1m0p', 'M_exp_m1m0p'}

    if ops & tiled_ops:
        return '2pass'
    else:
        return '3pass'


def generate_code(graph: ComputationGraph, strategy: str = 'fused') -> str:
    """Generate code from computation graph.

    Args:
        graph: Parsed computation graph
        strategy: 'composable', 'fused', or 'auto' (auto-detect algorithm)

    Returns:
        Generated Python/Triton code as string
    """
    if strategy == 'composable':
        codegen = ComposableCodeGen(graph)
    elif strategy == 'fused':
        # Auto-detect algorithm type
        algo = detect_algorithm(graph)
        if algo == '2pass':
            print(f"Detected 2-pass tiled attention algorithm")
            codegen = TwoPassCodeGen(graph)
        else:
            print(f"Detected 3-pass attention algorithm")
            codegen = FusedCodeGen(graph)
    elif strategy == 'auto':
        algo = detect_algorithm(graph)
        if algo == '2pass':
            codegen = TwoPassCodeGen(graph)
        else:
            codegen = FusedCodeGen(graph)
    else:
        raise ValueError(f"Unknown strategy: {strategy}")

    return codegen.generate()

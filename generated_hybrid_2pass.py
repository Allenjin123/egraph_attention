"""
Auto-generated attention implementation from egglog.
Generated by: egg_to_triton.py
"""

import torch
import triton
import triton.language as tl
import math
from typing import Optional



@triton.jit
def _attention_2pass_kernel(
    Q_ptr, K_ptr, V_ptr, Out_ptr,
    stride_qb, stride_qh, stride_qm, stride_qk,
    stride_kb, stride_kh, stride_kn, stride_kk,
    stride_vb, stride_vh, stride_vn, stride_vk,
    stride_ob, stride_oh, stride_om, stride_ok,
    batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
    scale,
    NUM_TILES: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """2-pass (FuseMax) attention kernel generated from egglog graph."""
    pid_batch = tl.program_id(0)
    pid_head = tl.program_id(1)
    pid_m = tl.program_id(2)

    # Query block offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_k = tl.arange(0, BLOCK_K)

    # Load Q block
    q_ptrs = Q_ptr + (pid_batch * stride_qb + pid_head * stride_qh +
                      offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)
    q_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    q = tl.load(q_ptrs, mask=q_mask, other=0.0)

    # === PASS 1: Find global max across all tiles ===
    global_max = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)

    for tile_idx in range(NUM_TILES):
        start_n = tile_idx * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K tile
        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # Compute QK for this tile
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Local max for this tile
        local_max = tl.max(qk, axis=1)

        # Local max computed by scaffold

        # Update global max
        global_max = tl.maximum(global_max, local_max)

        pass  # [HOLE: pass1_global_max] - empty

    # === PASS 2: Compute output using global max for correction ===
    global_sum = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_K], dtype=tl.float32)

    for tile_idx in range(NUM_TILES):
        start_n = tile_idx * BLOCK_N
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K tile (again)
        k_ptrs = K_ptr + (pid_batch * stride_kb + pid_head * stride_kh +
                          offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        k_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # Recompute QK
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk = tl.dot(q, tl.trans(k), qk)
        qk *= scale
        qk = tl.where(offs_n[None, :] < seq_len_k, qk, float('-inf'))

        # Local max for this tile
        local_max = tl.max(qk, axis=1)

        # Local softmax (numerically stable within tile)
        local_exp = tl.exp(qk - local_max[:, None])
        local_sum = tl.sum(local_exp, axis=1)

        pass  # [HOLE: pass2_local_softmax] - empty

        # Correction factor: exp(local_max - global_max)
        correction = tl.exp(local_max - global_max)

        # Correction factor: exp(local_max - global_max)

        # Corrected sum
        global_sum += local_sum * correction

        # Load V tile
        v_ptrs = V_ptr + (pid_batch * stride_vb + pid_head * stride_vh +
                          offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)
        v_mask = (offs_n[:, None] < seq_len_k) & (offs_k[None, :] < head_dim)
        v = tl.load(v_ptrs, mask=v_mask, other=0.0)

        # Corrected attention weights
        corrected_exp = local_exp * correction[:, None]
        acc += tl.dot(corrected_exp.to(v.dtype), v)

        pass  # [HOLE: pass2_accumulate] - empty

    # Final normalization
    acc = acc / global_sum[:, None]

    pass  # [HOLE: finalize] - empty

    # Store output
    out_ptrs = Out_ptr + (pid_batch * stride_ob + pid_head * stride_oh +
                          offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)
    out_mask = (offs_m[:, None] < seq_len_q) & (offs_k[None, :] < head_dim)
    tl.store(out_ptrs, acc.to(Out_ptr.dtype.element_ty), mask=out_mask)


def egg_attention_hybrid_2pass(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    scale: float = None,
) -> torch.Tensor:
    """
    Hybrid 2-pass attention implementation (scaffold + graph-derived ops).

    Args:
        q: Query tensor [batch, heads, seq_q, dim]
        k: Key tensor [batch, heads, seq_k, dim]
        v: Value tensor [batch, heads, seq_k, dim]
        scale: Scaling factor (default: 1/sqrt(dim))

    Returns:
        Output tensor [batch, heads, seq_q, dim]
    """
    batch_size, num_heads, seq_len_q, head_dim = q.shape
    _, _, seq_len_k, _ = k.shape

    if scale is None:
        scale = 1.0 / math.sqrt(head_dim)

    out = torch.empty_like(q)

    # Block sizes
    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = triton.next_power_of_2(head_dim)
    num_tiles = triton.cdiv(seq_len_k, BLOCK_N)

    # Grid
    grid = (batch_size, num_heads, triton.cdiv(seq_len_q, BLOCK_M))

    _attention_2pass_kernel[grid](
        q, k, v, out,
        q.stride(0), q.stride(1), q.stride(2), q.stride(3),
        k.stride(0), k.stride(1), k.stride(2), k.stride(3),
        v.stride(0), v.stride(1), v.stride(2), v.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        batch_size, num_heads, seq_len_q, seq_len_k, head_dim,
        scale,
        NUM_TILES=num_tiles,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return out



# ===== Test =====
if __name__ == "__main__":
    import torch

    torch.manual_seed(42)
    batch_size = 1
    num_heads = 4
    seq_len = 256
    head_dim = 64

    Q = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float16)
    K = torch.randn_like(Q)
    V = torch.randn_like(Q)

    # Run hybrid attention
    out = egg_attention_hybrid_2pass(Q, K, V)
    print(f"Output shape: {out.shape}")

    # Compare with PyTorch reference
    scale = 1.0 / math.sqrt(head_dim)
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1, dtype=torch.float32).to(Q.dtype)
    ref = torch.matmul(attn, V)

    diff = (out - ref).abs().max().item()
    print(f"Max diff vs reference: {diff:.6f}")
    if diff < 0.01:
        print("PASSED!")
    else:
        print("FAILED - outputs differ")

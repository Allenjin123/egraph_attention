"""
Graph-Driven Attention Implementation (PyTorch)
Generated by: pytorch_codegen.py

This implementation uses proper dimension-aware broadcasting computed
from the egglog computation graph.
"""

import torch
import math
from typing import Optional



# Tiling configuration - will be set dynamically based on sequence length
# Default: 4 tiles with tile size = seq_len // 4

def egg_attention_graphdriven(Q, K, V, scale=None):
    """
    Attention implementation generated from egglog computation graph.
    Uses dimension-aware broadcasting computed from graph.
    
    Args:
        Q: Query tensor [batch, heads, seq_q, dim] or [dim, seq]
        K: Key tensor [batch, heads, seq_k, dim] or [dim, seq]
        V: Value tensor [batch, heads, seq_k, dim] or [dim, seq]
        scale: Scaling factor (default: 1/sqrt(dim))
    
    Returns:
        Output tensor same shape as Q
    """

    # ===== Input Shape Handling =====
    original_shape = Q.shape
    if Q.dim() == 4:
        # [batch, heads, seq, dim] format
        batch_size, num_heads, seq_len, head_dim = Q.shape
        # Reshape to [batch*heads, dim, seq] for e/p dimensions
        Q = Q.permute(0, 1, 3, 2).reshape(-1, head_dim, seq_len)
        K = K.permute(0, 1, 3, 2).reshape(-1, head_dim, seq_len)
        V = V.permute(0, 1, 3, 2).reshape(-1, head_dim, seq_len)
        multi_head = True
    elif Q.dim() == 2:
        # [dim, seq] format - add batch dimension
        head_dim, seq_len = Q.shape
        batch_size, num_heads = 1, 1
        Q = Q.unsqueeze(0)
        K = K.unsqueeze(0)
        V = V.unsqueeze(0)
        multi_head = False
    else:
        raise ValueError(f"Unsupported input shape: {Q.shape}")

    if scale is None:
        scale = 1.0 / math.sqrt(head_dim)

    # ===== Computation Graph =====
    # Note: Working with [batch, e, p] layout where:
    #   - e (dim 1) = embedding/head dimension
    #   - p (dim 2) = query sequence position
    #   - m = key/value sequence position

    # Compute tile sizes: aim for 4 tiles by default
    _m_size = K.shape[-1]
    _num_tiles = min(4, _m_size)  # At most 4 tiles
    _tile_size = _m_size // _num_tiles
    T_split_m_m1m0 = K.unflatten(-1, (_num_tiles, _tile_size))  # output: [e, m1, m0]
    Q_bc = Q.unsqueeze(2).unsqueeze(3)
    T_split_m_m1m0_bc = T_split_m_m1m0.unsqueeze(4)
    M_mul_em1m0p = Q_bc * T_split_m_m1m0_bc  # output: [e, m1, m0, p]
    R_add_e = M_mul_em1m0p.sum(dim=1)
    R_add_e = R_add_e * scale  # Apply attention scale  # output: [m1, m0, p]
    R_max_m0 = R_add_e.max(dim=2)[0]  # output: [m1, p]
    R_max_m0_bc = R_max_m0.unsqueeze(2)
    M_sub_m1m0p = R_add_e - R_max_m0_bc  # output: [m1, m0, p]
    M_exp_m1m0p = torch.exp(M_sub_m1m0p)  # output: [m1, m0, p]
    R_max_m1 = R_max_m0.max(dim=1)[0]  # output: [p]
    R_max_m1_bc = R_max_m1.unsqueeze(1)
    M_sub_m1p = R_max_m0 - R_max_m1_bc  # output: [m1, p]
    M_exp_m1p = torch.exp(M_sub_m1p)  # output: [m1, p]
    M_exp_m1p_bc = M_exp_m1p.unsqueeze(2)
    M_mul_m1m0p = M_exp_m1m0p * M_exp_m1p_bc  # output: [m1, m0, p]
    R_add_m0 = M_exp_m1m0p.sum(dim=2)  # output: [m1, p]
    M_mul_m1p = R_add_m0 * M_exp_m1p  # output: [m1, p]
    R_add_m1 = M_mul_m1p.sum(dim=1)  # output: [p]
    R_add_m1_bc = R_add_m1.unsqueeze(1).unsqueeze(2)
    M_div_m1m0p = M_mul_m1m0p / R_add_m1_bc  # output: [m1, m0, p]
    T_unsplit_m1m0_m = M_div_m1m0p.flatten(1, 2)  # output: [m, p]
    T_unsplit_m1m0_m_bc = T_unsplit_m1m0_m.unsqueeze(1)
    V_bc = V.unsqueeze(3)
    M_mul_fmp = T_unsplit_m1m0_m_bc * V_bc  # output: [f, m, p]
    R_add_m = M_mul_fmp.sum(dim=2)  # output: [f, p]

    # ===== Output Reshape =====
    output = R_add_m
    if multi_head:
        # Reshape back to [batch, heads, seq, dim]
        output = output.reshape(batch_size, num_heads, head_dim, seq_len)
        output = output.permute(0, 1, 3, 2)
    else:
        output = output.squeeze(0)

    return output


# ===== Test =====
if __name__ == "__main__":
    import torch

    print("Testing graph-driven PyTorch attention...")
    print("=" * 60)

    torch.manual_seed(42)

    # Test 1: Simple 2D input
    print("\nTest 1: 2D input [dim, seq]")
    head_dim = 64
    seq_len = 128

    Q = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    K = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    V = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)

    out = egg_attention_graphdriven(Q, K, V)
    print(f"  Input shape: [{head_dim}, {seq_len}]")
    print(f"  Output shape: {list(out.shape)}")

    # Reference computation
    scale = 1.0 / math.sqrt(head_dim)
    scores = torch.matmul(Q.T, K) * scale  # [seq, seq]
    attn = torch.softmax(scores, dim=-1)
    ref = torch.matmul(attn, V.T).T  # [dim, seq]

    diff = (out - ref).abs().max().item()
    print(f"  Max diff vs reference: {diff:.6f}")
    print(f"  {'PASSED' if diff < 0.01 else 'FAILED'}!")

    # Test 2: 4D input (batch, heads)
    print("\nTest 2: 4D input [batch, heads, seq, dim]")
    batch_size = 2
    num_heads = 4
    seq_len = 256
    head_dim = 64

    Q = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float32)
    K = torch.randn_like(Q)
    V = torch.randn_like(Q)

    out = egg_attention_graphdriven(Q, K, V)
    print(f"  Input shape: {list(Q.shape)}")
    print(f"  Output shape: {list(out.shape)}")

    # Reference computation
    scale = 1.0 / math.sqrt(head_dim)
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1)
    ref = torch.matmul(attn, V)

    diff = (out - ref).abs().max().item()
    print(f"  Max diff vs reference: {diff:.6f}")
    print(f"  {'PASSED' if diff < 0.01 else 'FAILED'}!")

    print("\n" + "=" * 60)
    print("All tests completed!")

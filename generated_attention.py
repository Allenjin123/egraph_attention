"""
Auto-generated attention implementation from egglog.
Generated by: egg_to_triton.py
"""

import torch
import triton
import triton.language as tl
import math
from typing import Optional


def egg_attention_pytorch(Q, K, V, scale=None):
    """
    Attention implementation generated from egglog.
    Uses PyTorch operations for correctness verification.
    
    Args:
        Q: Query tensor [head_dim, seq_len] or [batch, heads, seq, dim]
        K: Key tensor [head_dim, seq_len]
        V: Value tensor [head_dim, seq_len]
        scale: Scaling factor (default: 1/sqrt(head_dim))
    """
    # Handle different input formats
    if Q.dim() == 4:
        # [batch, heads, seq, dim] -> process each head
        batch, heads, seq_len, head_dim = Q.shape
        Q = Q.permute(0, 1, 3, 2)  # [batch, heads, dim, seq]
        K = K.permute(0, 1, 3, 2)
        V = V.permute(0, 1, 3, 2)
        # Reshape for processing
        Q = Q.reshape(-1, head_dim, seq_len)  # [batch*heads, dim, seq]
        K = K.reshape(-1, head_dim, seq_len)
        V = V.reshape(-1, head_dim, seq_len)
        multi_head = True
    else:
        # Assume [dim, seq] format
        head_dim, seq_len = Q.shape
        batch, heads = 1, 1
        Q = Q.unsqueeze(0)
        K = K.unsqueeze(0)
        V = V.unsqueeze(0)
        multi_head = False

    if scale is None:
        scale = 1.0 / math.sqrt(head_dim)

    # ===== Computation Graph =====
    M_mul_emp = Q.unsqueeze(2) * K.unsqueeze(3)  # [batch, e, m, p]
    R_add_e = M_mul_emp.sum(dim=1)  # reduce e -> [batch, m, p]
    R_max_m = R_add_e.max(dim=1)[0]  # reduce m -> [batch, p]
    M_sub_mp = R_add_e - R_max_m.unsqueeze(1)  # broadcast subtract
    M_exp_mp = torch.exp(M_sub_mp)
    R_add_m = M_exp_mp.sum(dim=-2)  # reduce m
    M_div_mp = M_exp_mp / R_add_m.unsqueeze(-2)  # broadcast divide
    M_mul_fmp = M_div_mp.unsqueeze(1) * V.unsqueeze(3)  # [batch, f, m, p]
    R_add_m_1 = M_mul_fmp.sum(dim=-2)  # reduce m

    # ===== Output =====
    if multi_head:
        # Reshape back to [batch, heads, seq, dim]
        out = R_add_m_1.reshape(batch, heads, head_dim, seq_len)
        out = out.permute(0, 1, 3, 2)
        return out
    else:
        return R_add_m_1.squeeze(0)


# ===== Test =====
if __name__ == "__main__":
    import torch

    # Test with simple inputs
    torch.manual_seed(42)
    head_dim = 64
    seq_len = 128

    Q = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    K = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)
    V = torch.randn(head_dim, seq_len, device='cuda', dtype=torch.float32)

    # Run generated attention
    out = egg_attention_pytorch(Q, K, V)
    print(f"Output shape: {out.shape}")

    # Compare with reference
    scale = 1.0 / (head_dim ** 0.5)
    scores = torch.matmul(Q.T, K) * scale  # [seq, seq]
    attn = torch.softmax(scores, dim=-1)
    ref = torch.matmul(attn, V.T).T  # [dim, seq]

    diff = (out - ref).abs().max().item()
    print(f"Max diff vs reference: {diff:.6f}")
    if diff < 0.01:
        print("PASSED!")
    else:
        print("FAILED - outputs differ")
